{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter optimization\n",
    "Use Grid search to find best parameters for models. Use full dataset to train final models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from plots_fabi import *\n",
    "import pickle\n",
    "import sys\n",
    "# Add functions path\n",
    "sys.path.append('../../Functions')\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_stratified_dataset\n",
    "df = load_stratified_dataset(path='../../Datasets/dataset_categories/dataset_big.csv', labels='category', samples_per_label=1000, random_seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer generates bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sublinear_tf: use logarithmic form for frequency\n",
    "# min_df: minimum numbers of documents a word must be present to keep it\n",
    "# ngram_range: number of ngrams to use\n",
    "# stopwords: remove all common pronouns in given language\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1',\n",
    "                        ngram_range=(1, 3), stop_words=None, max_features=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_lem'], df['category'], random_state = 42)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# GridSearch\n",
    "Search for best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10.0, 'multi_class': 'ovr', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'penalty': ['l1', 'l2', 'elasticnet'], 'C': np.logspace(-1,1,10), 'multi_class': ['auto', 'ovr', 'multinomial']}\n",
    "clf = GridSearchCV(LogisticRegression(max_iter = 10000, random_state=42), parameters, n_jobs=4, cv=5)\n",
    "clf.fit(X_train_tfidf, y_train);\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.989904761904762, 0.7342857142857143)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_tfidf, y_train), clf.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.774263682681127,\n",
       " 'dual': False,\n",
       " 'loss': 'squared_hinge',\n",
       " 'penalty': 'l2'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'penalty': ['l1', 'l2'], 'C': np.logspace(-1,1,10), 'dual': [False, True], 'loss': ['hinge', 'squared_hinge']}\n",
    "clf = GridSearchCV(LinearSVC(max_iter = 10000, random_state=42), parameters, n_jobs=4, cv=5)\n",
    "clf.fit(X_train_tfidf, y_train);\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9891428571428571, 0.7308571428571429)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_tfidf, y_train), clf.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parameters = {'n_estimators': [100, 400, 700, 1000], 'criterion': ['gini','entropy'], 'max_depth': [None, 10, 30, 50, 80], 'min_samples_split': [2, 6, 10, 15]}\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=42), parameters, n_jobs=4, cv=5)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.score(X_train_tfidf, y_train), clf.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "For __all models__ grid search does __not__ really seem to __improve__ accuracy. Probably since errors occur due to the dataset given. Further grid analysis is therefor not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Full data optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest sample size in dataset is 12026 samples!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84181, 8), (3500, 8))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = load_stratified_dataset(path='../../Datasets/dataset_categories/dataset_categories_train.csv', labels='category', samples_per_label=99000, random_seed=42)\n",
    "df_test = pd.read_csv('../../Datasets/dataset_categories/dataset_categories_test.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = df_train.text_lem\n",
    "X_test = df_test.text_lem\n",
    "y_train = df_train.category\n",
    "y_test = df_test.category\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 74.37%;  Test accuracy: 74.94%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(nb.score(X_train_tfidf, y_train)*100, nb.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 92.54%;  Test accuracy: 82.94%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=10000, random_state=42, C=5.99, multi_class= 'ovr', penalty='l2')\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(lr.score(X_train_tfidf, y_train)*100, lr.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 92.29%;  Test accuracy: 83.06%\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(max_iter=10000, random_state=42, C=0.46, dual=False, loss='squared_hinge', penalty='l2')\n",
    "svc.fit(X_train_tfidf, y_train)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(svc.score(X_train_tfidf, y_train)*100, svc.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf = RandomForestClassifier(random_state=42, criterion='gini', max_depth=80, min_samples_split=6, n_estimators= 400)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(rf.score(X_train_tfidf, y_train)*100, rf.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voter = VotingClassifier(estimators=[('nb', nb), ('lr', lr), ('svc', svc), ('rf', rf)], voting='hard')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "voter.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(voter.score(X_train_tfidf, y_train)*100, voter.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Full data optimization with unbalanced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131549, 8), (3500, 8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../../Datasets/dataset_categories/dataset_categories_train.csv')\n",
    "df_test = pd.read_csv('../../Datasets/dataset_categories/dataset_categories_test.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "travel           1.000000\n",
       "world            1.000000\n",
       "sports           1.000000\n",
       "financial        1.000000\n",
       "technology       1.000000\n",
       "entertainment    1.024119\n",
       "politics         1.662980\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample weights for training\n",
    "cat_weight = 1/df_train.category.value_counts()*df_train.category.value_counts()[0]\n",
    "sample_weight = [cat_weight[i] for i in df_train.category]\n",
    "cat_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = df_train.text_lem\n",
    "y_train = df_train.category\n",
    "X_test = df_test.text_lem\n",
    "y_test = df_test.category\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 74.83%;  Test accuracy: 74.83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train, sample_weight=sample_weight)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(nb.score(X_train_tfidf, y_train)*100, nb.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 92.04%;  Test accuracy: 83.03%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=10000, random_state=42, C=5.99, multi_class= 'ovr', penalty='l2')\n",
    "lr.fit(X_train_tfidf, y_train, sample_weight=sample_weight)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(lr.score(X_train_tfidf, y_train)*100, lr.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 91.72%;  Test accuracy: 83.43%\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(max_iter=10000, random_state=42, C=0.46, dual=False, loss='squared_hinge', penalty='l2')\n",
    "svc.fit(X_train_tfidf, y_train, sample_weight = sample_weight)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(svc.score(X_train_tfidf, y_train)*100, svc.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf = RandomForestClassifier(random_state=42, criterion='gini', max_depth=80, min_samples_split=6, n_estimators= 400)\n",
    "rf.fit(X_train_tfidf, y_train, sample_weight = sample_weight)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(rf.score(X_train_tfidf, y_train)*100, rf.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp_feat(vectorizer, clf, X):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 40000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([word for word in X_test[0].split(' ') if word in tfidf.vocabulary_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(X_test[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 00',\n",
       " '00 car',\n",
       " '00 car sale',\n",
       " '00 cet',\n",
       " '00 company',\n",
       " '00 company mention',\n",
       " '00 eastern',\n",
       " '00 edt',\n",
       " '00 et',\n",
       " '00 local',\n",
       " '00 local real',\n",
       " '00 pm',\n",
       " '00 pm car',\n",
       " '00 pm edt',\n",
       " '00 pm seanna',\n",
       " '00 seanna',\n",
       " '00 seanna cronin',\n",
       " '000',\n",
       " '000 crore',\n",
       " '000 square',\n",
       " '000 square foot',\n",
       " '00am',\n",
       " '00p',\n",
       " '00pm',\n",
       " '01',\n",
       " '01 00',\n",
       " '01 15',\n",
       " '01 ed',\n",
       " '01 ed symkusmore',\n",
       " '01 pm',\n",
       " '01 update',\n",
       " '01am',\n",
       " '01am october',\n",
       " '02',\n",
       " '02 15',\n",
       " '02 pm',\n",
       " '03',\n",
       " '03 pm',\n",
       " '04',\n",
       " '04 pm',\n",
       " '05',\n",
       " '05 15',\n",
       " '05 pm',\n",
       " '06',\n",
       " '06 cet',\n",
       " '06 pm',\n",
       " '07',\n",
       " '07 15',\n",
       " '07 2015',\n",
       " '07 pm',\n",
       " '08',\n",
       " '08 15',\n",
       " '08 2015',\n",
       " '08 pm',\n",
       " '08 pm update',\n",
       " '08 update',\n",
       " '09',\n",
       " '09 15',\n",
       " '09 2015',\n",
       " '09 30',\n",
       " '09 pm',\n",
       " '09 pm update',\n",
       " '10',\n",
       " '10 00',\n",
       " '10 00 pm',\n",
       " '10 000',\n",
       " '10 01',\n",
       " '10 02',\n",
       " '10 03',\n",
       " '10 05',\n",
       " '10 07',\n",
       " '10 08',\n",
       " '10 08 update',\n",
       " '10 09',\n",
       " '10 10',\n",
       " '10 11',\n",
       " '10 12',\n",
       " '10 12 2015',\n",
       " '10 13',\n",
       " '10 14',\n",
       " '10 15',\n",
       " '10 16',\n",
       " '10 16 15',\n",
       " '10 17',\n",
       " '10 18',\n",
       " '10 19',\n",
       " '10 19 15',\n",
       " '10 19 2015',\n",
       " '10 20',\n",
       " '10 2015',\n",
       " '10 21',\n",
       " '10 21 2015',\n",
       " '10 22',\n",
       " '10 22 2015',\n",
       " '10 23',\n",
       " '10 24',\n",
       " '10 25',\n",
       " '10 26',\n",
       " '10 30',\n",
       " '10 32',\n",
       " '10 35',\n",
       " '10 38',\n",
       " '10 39',\n",
       " '10 40',\n",
       " '10 43',\n",
       " '10 45',\n",
       " '10 46',\n",
       " '10 49',\n",
       " '10 50',\n",
       " '10 54',\n",
       " '10 55',\n",
       " '10 9c',\n",
       " '10 day',\n",
       " '10 minute',\n",
       " '10 pm',\n",
       " '10 update',\n",
       " '10 yard',\n",
       " '10 year',\n",
       " '10 year bond',\n",
       " '10 year old',\n",
       " '10 year treasury',\n",
       " '10 year yield',\n",
       " '100',\n",
       " '100 height',\n",
       " '100 height 620',\n",
       " '100 yard',\n",
       " '100th',\n",
       " '1072n‘Å',\n",
       " '10k',\n",
       " '10th',\n",
       " '10th anniversary',\n",
       " '11',\n",
       " '11 00',\n",
       " '11 05',\n",
       " '11 08',\n",
       " '11 10',\n",
       " '11 11',\n",
       " '11 15',\n",
       " '11 20',\n",
       " '11 25',\n",
       " '11 28',\n",
       " '11 30',\n",
       " '11 31',\n",
       " '11 35',\n",
       " '11 39',\n",
       " '11 40',\n",
       " '11 45',\n",
       " '11 48',\n",
       " '11 50',\n",
       " '11 55',\n",
       " '11 59',\n",
       " '11 pm',\n",
       " '11 yard',\n",
       " '11 year',\n",
       " '11 year old',\n",
       " '1110n',\n",
       " '11th',\n",
       " '12',\n",
       " '12 00',\n",
       " '12 00 pm',\n",
       " '12 01',\n",
       " '12 01 update',\n",
       " '12 01am',\n",
       " '12 01am october',\n",
       " '12 05',\n",
       " '12 06',\n",
       " '12 08',\n",
       " '12 08 pm',\n",
       " '12 10',\n",
       " '12 11',\n",
       " '12 15',\n",
       " '12 20',\n",
       " '12 2015',\n",
       " '12 22',\n",
       " '12 28',\n",
       " '12 30',\n",
       " '12 31',\n",
       " '12 40',\n",
       " '12 45',\n",
       " '12 48',\n",
       " '12 59',\n",
       " '12 inch',\n",
       " '12 month',\n",
       " '12 pm',\n",
       " '12 yard',\n",
       " '12 year',\n",
       " '12 year old',\n",
       " '120',\n",
       " '12th',\n",
       " '13',\n",
       " '13 15',\n",
       " '13 pm',\n",
       " '13 pm update',\n",
       " '13 yard',\n",
       " '13 year',\n",
       " '13 year old',\n",
       " '130',\n",
       " '135',\n",
       " '13th',\n",
       " '13th oct',\n",
       " '14',\n",
       " '14 15',\n",
       " '14 pm',\n",
       " '14 yard',\n",
       " '14 year',\n",
       " '14 year old',\n",
       " '140',\n",
       " '14th',\n",
       " '14th oct',\n",
       " '15',\n",
       " '15 00',\n",
       " '15 01',\n",
       " '15 10',\n",
       " '15 11',\n",
       " '15 12',\n",
       " '15 12 01',\n",
       " '15 15',\n",
       " '15 2015',\n",
       " '15 30',\n",
       " '15 pm',\n",
       " '15 yard',\n",
       " '15 year',\n",
       " '15 year old',\n",
       " '150',\n",
       " '15pm',\n",
       " '15th',\n",
       " '16',\n",
       " '16 15',\n",
       " '16 2015',\n",
       " '16 pm',\n",
       " '16 year',\n",
       " '16 year old',\n",
       " '16th',\n",
       " '16th century',\n",
       " '17',\n",
       " '17 00',\n",
       " '17 15',\n",
       " '17 pm',\n",
       " '17 year',\n",
       " '17 year old',\n",
       " '17th',\n",
       " '17th century',\n",
       " '18',\n",
       " '18 15',\n",
       " '18 month',\n",
       " '18 pm',\n",
       " '18 yard',\n",
       " '18 year',\n",
       " '18 year old',\n",
       " '180',\n",
       " '18th',\n",
       " '18th century',\n",
       " '18th oct',\n",
       " '19',\n",
       " '19 15',\n",
       " '19 2015',\n",
       " '19 pm',\n",
       " '19 yard',\n",
       " '19 year',\n",
       " '19 year old',\n",
       " '1920s',\n",
       " '1930',\n",
       " '1940',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1990',\n",
       " '1990s',\n",
       " '19th',\n",
       " '19th century',\n",
       " '19th oct',\n",
       " '1a',\n",
       " '1b',\n",
       " '1bn',\n",
       " '1d',\n",
       " '1mdb',\n",
       " '1pc',\n",
       " '1st',\n",
       " '20',\n",
       " '20 15',\n",
       " '20 2015',\n",
       " '20 pm',\n",
       " '20 yard',\n",
       " '20 year',\n",
       " '20 year old',\n",
       " '200',\n",
       " '2000',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2015 00',\n",
       " '2015 00 company',\n",
       " '2015 05',\n",
       " '2015 07',\n",
       " '2015 08',\n",
       " '2015 09',\n",
       " '2015 10',\n",
       " '2015 11',\n",
       " '2015 12',\n",
       " '2015 ap',\n",
       " '2015 ap photo',\n",
       " '2016',\n",
       " '2016ers',\n",
       " '2016ers copy',\n",
       " '20th',\n",
       " '20th anniversary',\n",
       " '20th century',\n",
       " '20th century fox',\n",
       " '21',\n",
       " '21 15',\n",
       " '21 2015',\n",
       " '21 pm',\n",
       " '21 yard',\n",
       " '21 year',\n",
       " '21 year old',\n",
       " '21e',\n",
       " '21st',\n",
       " '21st century',\n",
       " '21st century fox',\n",
       " '21st oct',\n",
       " '22',\n",
       " '22 15',\n",
       " '22 2015',\n",
       " '22 pm',\n",
       " '22 yard',\n",
       " '22 year',\n",
       " '22 year old',\n",
       " '22nd',\n",
       " '22nd oct',\n",
       " '23',\n",
       " '23 00',\n",
       " '23 00 cet',\n",
       " '23 15',\n",
       " '23 2015',\n",
       " '23 fe',\n",
       " '23 pm',\n",
       " '23 yard',\n",
       " '23 year',\n",
       " '23 year old',\n",
       " '232',\n",
       " '23andme',\n",
       " '23rd',\n",
       " '23rd oct',\n",
       " '24',\n",
       " '24 2015',\n",
       " '24 hour',\n",
       " '24 pm',\n",
       " '24 yard',\n",
       " '24 year',\n",
       " '24 year old',\n",
       " '246',\n",
       " '249',\n",
       " '24th',\n",
       " '25',\n",
       " '25 15',\n",
       " '25 pm',\n",
       " '25 yard',\n",
       " '25 year',\n",
       " '25 year old',\n",
       " '250',\n",
       " '25th',\n",
       " '25th anniversary',\n",
       " '26',\n",
       " '26 pm',\n",
       " '26 yard',\n",
       " '26 year',\n",
       " '26 year old',\n",
       " '26th',\n",
       " '27',\n",
       " '27 pm',\n",
       " '27 yard',\n",
       " '27 year',\n",
       " '27 year old',\n",
       " '27e6c',\n",
       " '27th',\n",
       " '28',\n",
       " '28 2015',\n",
       " '28 nation',\n",
       " '28 pm',\n",
       " '28 year',\n",
       " '28 year old',\n",
       " '282',\n",
       " '282 de',\n",
       " '28th',\n",
       " '29',\n",
       " '29 2015',\n",
       " '29 pm',\n",
       " '29 year',\n",
       " '29 year old',\n",
       " '29th',\n",
       " '2a',\n",
       " '2bn',\n",
       " '2c',\n",
       " '2c6',\n",
       " '2ce',\n",
       " '2d',\n",
       " '2d e96',\n",
       " '2dd',\n",
       " '2de',\n",
       " '2e',\n",
       " '2e e96',\n",
       " '2e6',\n",
       " '2efc52j',\n",
       " '2j',\n",
       " '2j65',\n",
       " '2j6cd',\n",
       " '2jd',\n",
       " '2k',\n",
       " '2nd',\n",
       " '2nd oct',\n",
       " '2px',\n",
       " '2px solid',\n",
       " '2px solid ccc',\n",
       " '2q',\n",
       " '2q15',\n",
       " '30',\n",
       " '30 15',\n",
       " '30 2015',\n",
       " '30 30',\n",
       " '30 day',\n",
       " '30 day period',\n",
       " '30 eastern',\n",
       " '30 edt',\n",
       " '30 et',\n",
       " '30 friday',\n",
       " '30 minute',\n",
       " '30 pm',\n",
       " '30 saturday',\n",
       " '30 second',\n",
       " '30 wednesday',\n",
       " '30 yard',\n",
       " '30 year',\n",
       " '30 year old',\n",
       " '300',\n",
       " '300er',\n",
       " '300x250_sync',\n",
       " '30am',\n",
       " '30pm',\n",
       " '30pm channel',\n",
       " '30th',\n",
       " '30th anniversary',\n",
       " '31',\n",
       " '31 pm',\n",
       " '31 year',\n",
       " '31 year old',\n",
       " '31st',\n",
       " '32',\n",
       " '32 pm',\n",
       " '32 yard',\n",
       " '32 year',\n",
       " '32 year old',\n",
       " '33',\n",
       " '33 pm',\n",
       " '33 yard',\n",
       " '33 year',\n",
       " '33 year old',\n",
       " '33rd',\n",
       " '34',\n",
       " '34 pm',\n",
       " '34 yard',\n",
       " '34 year',\n",
       " '34 year old',\n",
       " '34th',\n",
       " '35',\n",
       " '35 pm',\n",
       " '35 yard',\n",
       " '35 year',\n",
       " '35 year old',\n",
       " '35th',\n",
       " '36',\n",
       " '36 cet',\n",
       " '36 pm',\n",
       " '36 yard',\n",
       " '36 year',\n",
       " '36 year old',\n",
       " '360',\n",
       " '360 degree',\n",
       " '3642fd6',\n",
       " '367',\n",
       " '367 c6',\n",
       " '36c',\n",
       " '36de',\n",
       " '36ee6c',\n",
       " '36th',\n",
       " '37',\n",
       " '37 pm',\n",
       " '37 pm update',\n",
       " '37 yard',\n",
       " '37 year',\n",
       " '37 year old',\n",
       " '37th',\n",
       " '38',\n",
       " '38 pm',\n",
       " '38 year',\n",
       " '38 year old',\n",
       " '38th',\n",
       " '39',\n",
       " '39 pm',\n",
       " '39 year',\n",
       " '39 year old',\n",
       " '3a',\n",
       " '3bn',\n",
       " '3c',\n",
       " '3d',\n",
       " '3d print',\n",
       " '3d printer',\n",
       " '3d printing',\n",
       " '3d touch',\n",
       " '3f',\n",
       " '3fe',\n",
       " '3j',\n",
       " '3pc',\n",
       " '3rd',\n",
       " '40',\n",
       " '40 pm',\n",
       " '40 pm post',\n",
       " '40 yard',\n",
       " '40 year',\n",
       " '40 year old',\n",
       " '400',\n",
       " '401',\n",
       " '40th',\n",
       " '40th anniversary',\n",
       " '41',\n",
       " '41 pm',\n",
       " '41 yard',\n",
       " '41 year',\n",
       " '41 year old',\n",
       " '41st',\n",
       " '42',\n",
       " '42 pm',\n",
       " '42 yard',\n",
       " '42 year',\n",
       " '42 year old',\n",
       " '42nd',\n",
       " '43',\n",
       " '43 pm',\n",
       " '43 year',\n",
       " '43 year old',\n",
       " '43rd',\n",
       " '44',\n",
       " '44 pm',\n",
       " '44 year',\n",
       " '44 year old',\n",
       " '44th',\n",
       " '45',\n",
       " '45 pm',\n",
       " '45 year',\n",
       " '45 year old',\n",
       " '45pm',\n",
       " '45th',\n",
       " '46',\n",
       " '46 pm',\n",
       " '46 year',\n",
       " '46 year old',\n",
       " '46d',\n",
       " '47',\n",
       " '47 pm',\n",
       " '47 year',\n",
       " '47 year old',\n",
       " '48',\n",
       " '48 pm',\n",
       " '48 year',\n",
       " '48 year old',\n",
       " '49',\n",
       " '49 pm',\n",
       " '492',\n",
       " '49er',\n",
       " '49ers',\n",
       " '4a',\n",
       " '4bn',\n",
       " '4c',\n",
       " '4d',\n",
       " '4e',\n",
       " '4f',\n",
       " '4k',\n",
       " '4k video',\n",
       " '4th',\n",
       " '50',\n",
       " '50 pm',\n",
       " '50 yard',\n",
       " '50 year',\n",
       " '50 year old',\n",
       " '500',\n",
       " '501',\n",
       " '50p',\n",
       " '50th',\n",
       " '50th anniversary',\n",
       " '51',\n",
       " '51 pm',\n",
       " '51 year',\n",
       " '51 year old',\n",
       " '52',\n",
       " '52 pm',\n",
       " '52 week',\n",
       " '52 week high',\n",
       " '52 yard',\n",
       " '52j',\n",
       " '53',\n",
       " '53 pm',\n",
       " '54',\n",
       " '54 pm',\n",
       " '54 year',\n",
       " '54 year old',\n",
       " '55',\n",
       " '55 pm',\n",
       " '55 update',\n",
       " '56',\n",
       " '56 pm',\n",
       " '56 year',\n",
       " '565',\n",
       " '5676',\n",
       " '5676 d6',\n",
       " '56c',\n",
       " '56cd',\n",
       " '57',\n",
       " '57 pm',\n",
       " '58',\n",
       " '58 pm',\n",
       " '59',\n",
       " '59 pm',\n",
       " '5a',\n",
       " '5bn',\n",
       " '5c',\n",
       " '5d',\n",
       " '5fc',\n",
       " '5j',\n",
       " '5k',\n",
       " '5pc',\n",
       " '5s',\n",
       " '5th',\n",
       " '5x',\n",
       " '60',\n",
       " '60 year',\n",
       " '600',\n",
       " '60th',\n",
       " '61',\n",
       " '62',\n",
       " '62 year',\n",
       " '62 year old',\n",
       " '620',\n",
       " '620 frameborder',\n",
       " '620 frameborder style',\n",
       " '62c',\n",
       " '63',\n",
       " '64',\n",
       " '64 bit',\n",
       " '64e',\n",
       " '65',\n",
       " '65 e96',\n",
       " '65 year',\n",
       " '65 year old',\n",
       " '66',\n",
       " '66 year',\n",
       " '66 year old',\n",
       " '66a',\n",
       " '66th',\n",
       " '67',\n",
       " '67e',\n",
       " '68',\n",
       " '69',\n",
       " '6a',\n",
       " '6bn',\n",
       " '6c',\n",
       " '6cd',\n",
       " '6d',\n",
       " '6d am',\n",
       " '6d e96',\n",
       " '6dd',\n",
       " '6de',\n",
       " '6e',\n",
       " '6g6',\n",
       " '6g6c',\n",
       " '6h',\n",
       " '6ie',\n",
       " '6j',\n",
       " '6p',\n",
       " '6s',\n",
       " '6s 6s',\n",
       " '6s 6s plus',\n",
       " '6s iphone',\n",
       " '6s iphone 6s',\n",
       " '6s plus',\n",
       " '6th',\n",
       " '70',\n",
       " '70 year',\n",
       " '700',\n",
       " '70th',\n",
       " '70th anniversary',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '75 year',\n",
       " '75th',\n",
       " '76',\n",
       " '77',\n",
       " '776',\n",
       " '776 d6',\n",
       " '78',\n",
       " '78 year',\n",
       " '78 year old',\n",
       " '79',\n",
       " '79 year',\n",
       " '7a',\n",
       " '7bn',\n",
       " '7c',\n",
       " '7c e96',\n",
       " '7f',\n",
       " '7th',\n",
       " '80',\n",
       " '80 yard',\n",
       " '800',\n",
       " '802',\n",
       " '81',\n",
       " '82',\n",
       " '82 6d',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '86c',\n",
       " '86e',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '89e',\n",
       " '8bn',\n",
       " '8c',\n",
       " '8c2',\n",
       " '8c62e',\n",
       " '8d',\n",
       " '8m',\n",
       " '8mk',\n",
       " '8mk am',\n",
       " '8th',\n",
       " '8th oct',\n",
       " '8th oct 00',\n",
       " '90',\n",
       " '90 day',\n",
       " '90 minute',\n",
       " '91',\n",
       " '92',\n",
       " '92d',\n",
       " '92e',\n",
       " '92g6',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '959f',\n",
       " '96',\n",
       " '96c',\n",
       " '96c6',\n",
       " '96j',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '9c',\n",
       " '9c67lq9eeai',\n",
       " '9th',\n",
       " '_blank',\n",
       " '_blank img',\n",
       " '_blank img www',\n",
       " '_fw_input_cnn_300x250_sync_freewheel',\n",
       " '_fw_input_cnn_300x250_sync_freewheel rail_0',\n",
       " 'a2',\n",
       " 'a2ce',\n",
       " 'a2dd',\n",
       " 'a3',\n",
       " 'a320',\n",
       " 'a330',\n",
       " 'a350',\n",
       " 'a350 xwb',\n",
       " 'a380',\n",
       " 'a6',\n",
       " 'a6c',\n",
       " 'a6c46',\n",
       " 'a9',\n",
       " 'a_',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aac',\n",
       " 'aadhaar',\n",
       " 'aal',\n",
       " 'aamer',\n",
       " 'aamir',\n",
       " 'aap',\n",
       " 'aap minute',\n",
       " 'aap minute ago',\n",
       " 'aapl',\n",
       " 'aaron',\n",
       " 'aaron rodgers',\n",
       " 'ab',\n",
       " 'ab inbev',\n",
       " 'ababa',\n",
       " 'abandon',\n",
       " 'abate',\n",
       " 'abbas',\n",
       " 'abbey',\n",
       " 'abbott',\n",
       " 'abbott government',\n",
       " 'abbott say',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abc family',\n",
       " 'abc news',\n",
       " 'abd',\n",
       " 'abdel',\n",
       " 'abdel fattah',\n",
       " 'abduct',\n",
       " 'abduction',\n",
       " 'abdul',\n",
       " 'abdullah',\n",
       " 'abe',\n",
       " 'abedin',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'abi',\n",
       " 'abide',\n",
       " 'abide site',\n",
       " 'abide site term',\n",
       " 'abigail',\n",
       " 'abila',\n",
       " 'ability',\n",
       " 'ability deliver',\n",
       " 'ability provide',\n",
       " 'able',\n",
       " 'able access',\n",
       " 'able bodied',\n",
       " 'able bring',\n",
       " 'able come',\n",
       " 'able deliver',\n",
       " 'able enjoy',\n",
       " 'able find',\n",
       " 'able handle',\n",
       " 'able help',\n",
       " 'able meet',\n",
       " 'able offer',\n",
       " 'able pay',\n",
       " 'able play',\n",
       " 'able provide',\n",
       " 'able reach',\n",
       " 'able run',\n",
       " 'able sit',\n",
       " 'able use',\n",
       " 'able work',\n",
       " 'abnormal',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'aboriginal',\n",
       " 'abortion',\n",
       " 'abound',\n",
       " 'about',\n",
       " 'abraham',\n",
       " 'abrams',\n",
       " 'abreast',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorb',\n",
       " 'absorption',\n",
       " 'abstain',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'abta',\n",
       " 'abu',\n",
       " 'abu dhabi',\n",
       " 'abuja',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abuser',\n",
       " 'abusive',\n",
       " 'abusive malicious',\n",
       " 'abusive malicious wish',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'academy award',\n",
       " 'academy sciences',\n",
       " 'acc',\n",
       " 'accc',\n",
       " 'accelerate',\n",
       " 'accelerate growth',\n",
       " 'accelerated',\n",
       " 'acceleration',\n",
       " 'accelerator',\n",
       " 'accent',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'accept refugee',\n",
       " 'accept responsibility',\n",
       " 'accept term',\n",
       " 'accept term condition',\n",
       " 'accept term visitor',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'access',\n",
       " 'access control',\n",
       " 'access datum',\n",
       " 'access good',\n",
       " 'access information',\n",
       " 'access internet',\n",
       " 'access new',\n",
       " 'access point',\n",
       " 'access profile',\n",
       " 'access profile editor',\n",
       " 'access road',\n",
       " 'access service',\n",
       " 'access timely',\n",
       " 'access timely news',\n",
       " 'access week',\n",
       " 'access yahoo',\n",
       " 'access yahoo site',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclimate',\n",
       " 'accolade',\n",
       " 'accommodate',\n",
       " 'accommodation',\n",
       " 'accompany',\n",
       " 'accompany vip',\n",
       " 'accompany vip suite',\n",
       " 'accomplice',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'accor',\n",
       " 'accord',\n",
       " 'accord abc',\n",
       " 'accord analysis',\n",
       " 'accord analyst',\n",
       " 'accord authority',\n",
       " 'accord bloomberg',\n",
       " 'accord company',\n",
       " 'accord court',\n",
       " 'accord court document',\n",
       " 'accord datum',\n",
       " 'accord datum compile',\n",
       " 'accord document',\n",
       " 'accord figure',\n",
       " 'accord government',\n",
       " 'accord hollywood',\n",
       " 'accord hollywood reporter',\n",
       " 'accord international',\n",
       " 'accord late',\n",
       " 'accord local',\n",
       " 'accord mr',\n",
       " 'accord national',\n",
       " 'accord new',\n",
       " 'accord new report',\n",
       " 'accord new study',\n",
       " 'accord news',\n",
       " 'accord official',\n",
       " 'accord people',\n",
       " 'accord people familiar',\n",
       " 'accord person',\n",
       " 'accord police',\n",
       " 'accord recent',\n",
       " 'accord report',\n",
       " 'accord research',\n",
       " 'accord reuters',\n",
       " 'accord source',\n",
       " 'accord stat',\n",
       " 'accord state',\n",
       " 'accord statement',\n",
       " 'accord study',\n",
       " 'accord survey',\n",
       " 'accord thewrap',\n",
       " 'accord thomson',\n",
       " 'accord thomson reuters',\n",
       " 'accord tmz',\n",
       " 'accord website',\n",
       " 'accord world',\n",
       " 'accordance',\n",
       " 'accordingly',\n",
       " 'accorhotel',\n",
       " 'account',\n",
       " 'account beer',\n",
       " 'account beer duty',\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Save final model\n",
    "Save final model to use it on streamlit. Save the tfidf vectorizer and the svc model as pkl files. Write a class to load both models and use them to categorize final text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from text_lemmatization import Lemmatizer\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use smaller model, else size of files too big for streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer generates bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sublinear_tf: use logarithmic form for frequency\n",
    "# min_df: minimum numbers of documents a word must be present to keep it\n",
    "# ngram_range: number of ngrams to use\n",
    "# stopwords: remove all common pronouns in given language\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1',\n",
    "                        ngram_range=(1, 3), max_features=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_stratified_dataset(path='../../Datasets/dataset_categories/dataset_categories_train.csv', labels='category', samples_per_label=100000, random_seed=42)\n",
    "df_test = pd.read_csv('../../Datasets/dataset_categories/dataset_categories_test.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = df_train.text_lem\n",
    "X_test = df_test.text_lem\n",
    "y_train = df_train.category\n",
    "y_test = df_test.category\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(max_iter=10000, random_state=42, C=0.46, dual=False, loss='squared_hinge', penalty='l2')\n",
    "svc.fit(X_train_tfidf, y_train)\n",
    "print('Training accuracy: {:.2f}%;  Test accuracy: {:.2f}%'.format(svc.score(X_train_tfidf, y_train)*100, svc.score(X_test_tfidf, y_test)*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(tfidf.vocabulary_) 977601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delattr(tfidf, 'stop_words')\n",
    "delattr(tfidf, 'stop_words_')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "dir(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf, open(\"tfidf.pkl\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL) # 378M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh tfidf.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svc, open(\"svc.pkl\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh svc.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorizer():\n",
    "    def __init__(self):\n",
    "        self.tfidf = pickle.load(open('tfidf.pkl', 'rb'))\n",
    "        #self.lemmatizer = lemmatizer\n",
    "        self.svc = pickle.load(open('svc.pkl', 'rb'))\n",
    "    def preprocess(self, X):\n",
    "        \n",
    "        # Check if X is string, turn to list\n",
    "        if type(X) == str:\n",
    "            X = [X]\n",
    "                    \n",
    "        # Lemmatization\n",
    "        #X_lem = [self.lemmatizer.lem_text(x) for x in X]\n",
    "                \n",
    "        # Tfidf vectorization\n",
    "        X_tfidf = self.tfidf.transform(X)\n",
    "        \n",
    "        return X_tfidf\n",
    "    \n",
    "    def pred(self, X):\n",
    "        \n",
    "        # preprocess\n",
    "        X_tfidf = self.preprocess(X)\n",
    "        \n",
    "        # return categories\n",
    "        return self.svc.predict(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorizer = Categorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = categorizer.pred(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred == y_test).sum()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracy is correct, loading and using model works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
